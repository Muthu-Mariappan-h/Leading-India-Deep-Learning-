{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 1: Convolutional Neural Network"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "###  Importing packages"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.models import Model\nimport timeit\n\nimport warnings\nwarnings.filterwarnings('ignore')",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "['__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n 'get_file',\n 'load_data',\n 'np']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Preparing Dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "batch_size = 128\nnum_classes = 10\nepochs = 1\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11247616/11490434 [============================>.] - ETA: 0sx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Building a Model"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(16, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 13, 13, 8)         0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 11, 11, 16)        1168      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                12832     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 14,410\nTrainable params: 14,410\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(16, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Model Training"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 21s - loss: 0.3922 - acc: 0.8758 - val_loss: 0.1244 - val_acc: 0.9625\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f538aeaa438>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Testing"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Test loss: 0.1244291312545538\nTest accuracy: 0.9625\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Prediction"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pylab as plt\n\nplt.imshow(x_test[333].reshape(28,28),cmap='gray')\nplt.show()",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7f538018a4a8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nprediction = model.predict(x_test[333:334])\nprint('Prediction Score:\\n',prediction[0])\nthresholded = (prediction>0.5)*1\nprint('\\nThresholded Score:\\n',thresholded[0])\nprint('\\nPredicted Digit:\\n',np.where(thresholded == 1)[1][0])",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Prediction Score:\n [1.46515486e-05 2.04339745e-09 1.09278135e-05 2.46997532e-02\n 1.71028933e-08 9.49109554e-01 3.28120734e-07 3.15602108e-07\n 2.59720422e-02 1.92417589e-04]\n\nThresholded Score:\n [0 0 0 0 0 1 0 0 0 0]\n\nPredicted Digit:\n 5\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 2: Applications of Convolutional Neural Network"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "###  MobileNet Models"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = MobileNet(input_shape=None, alpha=0.25, depth_multiplier=1, dropout=1e-3, \n                                 include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\n\nmodel.summary()",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nconv1 (Conv2D)               (None, 112, 112, 8)       216       \n_________________________________________________________________\nconv1_bn (BatchNormalization (None, 112, 112, 8)       32        \n_________________________________________________________________\nconv1_relu (Activation)      (None, 112, 112, 8)       0         \n_________________________________________________________________\nconv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 8)       72        \n_________________________________________________________________\nconv_dw_1_bn (BatchNormaliza (None, 112, 112, 8)       32        \n_________________________________________________________________\nconv_dw_1_relu (Activation)  (None, 112, 112, 8)       0         \n_________________________________________________________________\nconv_pw_1 (Conv2D)           (None, 112, 112, 16)      128       \n_________________________________________________________________\nconv_pw_1_bn (BatchNormaliza (None, 112, 112, 16)      64        \n_________________________________________________________________\nconv_pw_1_relu (Activation)  (None, 112, 112, 16)      0         \n_________________________________________________________________\nconv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 16)        144       \n_________________________________________________________________\nconv_dw_2_bn (BatchNormaliza (None, 56, 56, 16)        64        \n_________________________________________________________________\nconv_dw_2_relu (Activation)  (None, 56, 56, 16)        0         \n_________________________________________________________________\nconv_pw_2 (Conv2D)           (None, 56, 56, 32)        512       \n_________________________________________________________________\nconv_pw_2_bn (BatchNormaliza (None, 56, 56, 32)        128       \n_________________________________________________________________\nconv_pw_2_relu (Activation)  (None, 56, 56, 32)        0         \n_________________________________________________________________\nconv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 32)        288       \n_________________________________________________________________\nconv_dw_3_bn (BatchNormaliza (None, 56, 56, 32)        128       \n_________________________________________________________________\nconv_dw_3_relu (Activation)  (None, 56, 56, 32)        0         \n_________________________________________________________________\nconv_pw_3 (Conv2D)           (None, 56, 56, 32)        1024      \n_________________________________________________________________\nconv_pw_3_bn (BatchNormaliza (None, 56, 56, 32)        128       \n_________________________________________________________________\nconv_pw_3_relu (Activation)  (None, 56, 56, 32)        0         \n_________________________________________________________________\nconv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 32)        288       \n_________________________________________________________________\nconv_dw_4_bn (BatchNormaliza (None, 28, 28, 32)        128       \n_________________________________________________________________\nconv_dw_4_relu (Activation)  (None, 28, 28, 32)        0         \n_________________________________________________________________\nconv_pw_4 (Conv2D)           (None, 28, 28, 64)        2048      \n_________________________________________________________________\nconv_pw_4_bn (BatchNormaliza (None, 28, 28, 64)        256       \n_________________________________________________________________\nconv_pw_4_relu (Activation)  (None, 28, 28, 64)        0         \n_________________________________________________________________\nconv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 64)        576       \n_________________________________________________________________\nconv_dw_5_bn (BatchNormaliza (None, 28, 28, 64)        256       \n_________________________________________________________________\nconv_dw_5_relu (Activation)  (None, 28, 28, 64)        0         \n_________________________________________________________________\nconv_pw_5 (Conv2D)           (None, 28, 28, 64)        4096      \n_________________________________________________________________\nconv_pw_5_bn (BatchNormaliza (None, 28, 28, 64)        256       \n_________________________________________________________________\nconv_pw_5_relu (Activation)  (None, 28, 28, 64)        0         \n_________________________________________________________________\nconv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 64)        576       \n_________________________________________________________________\nconv_dw_6_bn (BatchNormaliza (None, 14, 14, 64)        256       \n_________________________________________________________________\nconv_dw_6_relu (Activation)  (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv_pw_6 (Conv2D)           (None, 14, 14, 128)       8192      \n_________________________________________________________________\nconv_pw_6_bn (BatchNormaliza (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_pw_6_relu (Activation)  (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 128)       1152      \n_________________________________________________________________\nconv_dw_7_bn (BatchNormaliza (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_dw_7_relu (Activation)  (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_pw_7 (Conv2D)           (None, 14, 14, 128)       16384     \n_________________________________________________________________\nconv_pw_7_bn (BatchNormaliza (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_pw_7_relu (Activation)  (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 128)       1152      \n_________________________________________________________________\nconv_dw_8_bn (BatchNormaliza (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_dw_8_relu (Activation)  (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_pw_8 (Conv2D)           (None, 14, 14, 128)       16384     \n_________________________________________________________________\nconv_pw_8_bn (BatchNormaliza (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_pw_8_relu (Activation)  (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 128)       1152      \n_________________________________________________________________\nconv_dw_9_bn (BatchNormaliza (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_dw_9_relu (Activation)  (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_pw_9 (Conv2D)           (None, 14, 14, 128)       16384     \n_________________________________________________________________\nconv_pw_9_bn (BatchNormaliza (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_pw_9_relu (Activation)  (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_dw_10 (DepthwiseConv2D) (None, 14, 14, 128)       1152      \n_________________________________________________________________\nconv_dw_10_bn (BatchNormaliz (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_dw_10_relu (Activation) (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_pw_10 (Conv2D)          (None, 14, 14, 128)       16384     \n_________________________________________________________________\nconv_pw_10_bn (BatchNormaliz (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_pw_10_relu (Activation) (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_dw_11 (DepthwiseConv2D) (None, 14, 14, 128)       1152      \n_________________________________________________________________\nconv_dw_11_bn (BatchNormaliz (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_dw_11_relu (Activation) (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_pw_11 (Conv2D)          (None, 14, 14, 128)       16384     \n_________________________________________________________________\nconv_pw_11_bn (BatchNormaliz (None, 14, 14, 128)       512       \n_________________________________________________________________\nconv_pw_11_relu (Activation) (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv_dw_12 (DepthwiseConv2D) (None, 7, 7, 128)         1152      \n_________________________________________________________________\nconv_dw_12_bn (BatchNormaliz (None, 7, 7, 128)         512       \n_________________________________________________________________\nconv_dw_12_relu (Activation) (None, 7, 7, 128)         0         \n_________________________________________________________________\nconv_pw_12 (Conv2D)          (None, 7, 7, 256)         32768     \n_________________________________________________________________\nconv_pw_12_bn (BatchNormaliz (None, 7, 7, 256)         1024      \n_________________________________________________________________\nconv_pw_12_relu (Activation) (None, 7, 7, 256)         0         \n_________________________________________________________________\nconv_dw_13 (DepthwiseConv2D) (None, 7, 7, 256)         2304      \n_________________________________________________________________\nconv_dw_13_bn (BatchNormaliz (None, 7, 7, 256)         1024      \n_________________________________________________________________\nconv_dw_13_relu (Activation) (None, 7, 7, 256)         0         \n_________________________________________________________________\nconv_pw_13 (Conv2D)          (None, 7, 7, 256)         65536     \n_________________________________________________________________\nconv_pw_13_bn (BatchNormaliz (None, 7, 7, 256)         1024      \n_________________________________________________________________\nconv_pw_13_relu (Activation) (None, 7, 7, 256)         0         \n_________________________________________________________________\nglobal_average_pooling2d_6 ( (None, 256)               0         \n_________________________________________________________________\nreshape_1 (Reshape)          (None, 1, 1, 256)         0         \n_________________________________________________________________\ndropout (Dropout)            (None, 1, 1, 256)         0         \n_________________________________________________________________\nconv_preds (Conv2D)          (None, 1, 1, 1000)        257000    \n_________________________________________________________________\nact_softmax (Activation)     (None, 1, 1, 1000)        0         \n_________________________________________________________________\nreshape_2 (Reshape)          (None, 1000)              0         \n=================================================================\nTotal params: 475,544\nTrainable params: 470,072\nNon-trainable params: 5,472\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "###  Classify images"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Write the image name below\n\nimg_path = 'digit.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = model.predict(x)\nprint('Predicted:\\n', decode_predictions(preds))",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n 8192/35363 [=====>........................] - ETA: 0sPredicted:\n [[('n04286575', 'spotlight', 0.12874684), ('n03729826', 'matchstick', 0.1255584), ('n03666591', 'lighter', 0.07316567), ('n03804744', 'nail', 0.046445183), ('n01833805', 'hummingbird', 0.028862849)]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "###  Extract CNN features"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "features = model.predict(x)\nprint('\\nFeature Shape:\\n',features.shape)\nprint('\\nFeatures:\\n',features)",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\nFeature Shape:\n (1, 1000)\n\nFeatures:\n [[5.09005417e-07 5.94669546e-05 1.22427635e-04 6.39713035e-05\n  1.59262458e-03 1.24232247e-05 2.77951021e-05 1.87593669e-05\n  8.02526756e-06 2.07948106e-05 4.91983155e-06 4.08512187e-06\n  1.52589905e-06 1.60167365e-05 5.71169985e-05 7.40686801e-07\n  1.56031223e-04 1.19447919e-04 2.47570314e-03 6.10591902e-04\n  8.92563767e-05 4.35892871e-04 2.58735032e-04 3.67143512e-04\n  5.16555565e-06 2.27297369e-06 1.61253083e-05 1.48667386e-05\n  1.93651922e-05 8.56797051e-05 1.22397500e-06 3.10445903e-04\n  6.87005668e-05 2.66890966e-05 8.40093708e-05 5.35501749e-06\n  6.48538044e-05 1.43450245e-06 4.29758755e-03 5.35786330e-06\n  9.46434098e-04 1.66753998e-05 4.81312418e-06 1.96782101e-04\n  3.04559944e-05 7.42519262e-07 5.33681945e-04 4.93123196e-04\n  3.87757126e-07 4.65445919e-06 5.02027387e-06 2.64343180e-05\n  2.16324206e-05 2.74038721e-05 6.43116482e-06 1.85557437e-05\n  2.55284598e-04 4.49047235e-08 2.38023131e-05 8.60265791e-05\n  6.81418169e-05 4.63249222e-07 2.85418764e-06 4.18248828e-06\n  3.92561815e-05 1.40547476e-04 1.47658930e-05 9.80949790e-06\n  1.77304137e-05 3.99774848e-07 1.88434694e-06 4.68934566e-04\n  1.16858806e-04 5.89480216e-04 7.55596702e-05 4.04860511e-05\n  1.39547456e-05 2.16210374e-05 5.60163971e-05 5.29034878e-05\n  7.07563813e-05 1.09248453e-04 3.18437560e-05 4.92155823e-06\n  3.82625922e-06 8.52969697e-06 1.83166912e-06 4.47416125e-04\n  5.29955423e-06 5.40601963e-04 2.35844013e-07 2.37563454e-05\n  1.43380157e-05 5.46699557e-05 2.88628489e-02 1.00748148e-04\n  1.53847272e-04 3.33589887e-05 1.48315612e-05 1.00762036e-03\n  1.95015600e-05 1.66899792e-06 1.00289878e-06 3.08142371e-05\n  2.12022805e-05 1.24482654e-06 9.16112754e-07 8.33694794e-05\n  3.21384255e-06 5.57690356e-08 6.66120555e-04 1.45553437e-04\n  3.04627768e-03 2.84489273e-04 3.89198132e-04 1.27422961e-03\n  1.16404988e-06 2.54677521e-04 1.86150302e-07 3.58940929e-06\n  6.57230557e-05 1.40995314e-06 3.06043808e-06 1.59775865e-07\n  4.02717706e-06 1.87112411e-04 1.17413671e-04 2.09871767e-04\n  7.45414727e-05 2.62094289e-03 1.44845762e-05 1.21848307e-05\n  1.59130525e-03 1.30885758e-06 3.59983766e-04 1.05495542e-06\n  1.02354807e-05 2.33922747e-05 2.13252297e-05 1.92590887e-05\n  4.34394360e-06 3.32343029e-06 4.38241813e-07 3.50407972e-05\n  5.97008446e-04 4.28946760e-06 6.91549911e-04 4.40325402e-06\n  1.54944821e-04 3.95919460e-06 7.28511907e-07 2.62627582e-04\n  8.32011749e-04 1.75577225e-05 2.60263787e-05 3.13462697e-05\n  6.01987231e-05 1.20482850e-03 2.39913672e-04 3.42832573e-06\n  2.46963064e-05 2.87111252e-05 8.71530938e-05 1.65332392e-06\n  6.33749733e-05 1.07737333e-06 2.51616380e-04 4.71784078e-05\n  1.14557624e-05 2.19957787e-04 1.61515800e-05 1.76079280e-04\n  1.15322029e-04 4.94850276e-04 2.11148890e-05 3.73038461e-06\n  4.88644255e-05 1.13042232e-04 7.30638521e-06 1.60695126e-04\n  8.86257403e-05 1.82081611e-04 1.61696571e-05 2.52679951e-04\n  1.23093778e-04 2.27436467e-05 3.15765305e-06 1.14151935e-05\n  6.16698002e-04 2.24456453e-04 4.83234253e-05 7.82154439e-06\n  1.80979005e-05 1.30552905e-06 2.31991385e-06 4.25643986e-04\n  8.63739406e-05 3.05990543e-05 9.51950369e-06 7.38969902e-05\n  3.87692089e-05 4.71635394e-06 5.92682845e-05 1.16165567e-04\n  3.02837507e-05 2.45609008e-05 1.60372001e-05 1.05025783e-05\n  3.27565976e-05 7.95696906e-06 5.59248547e-05 6.61830518e-06\n  9.10164599e-05 5.63783942e-06 1.84018177e-06 5.39615809e-04\n  5.77928085e-06 1.86406134e-04 8.77501152e-05 2.05068845e-05\n  4.61326817e-06 3.47687383e-05 7.32459521e-05 2.17373934e-04\n  7.30838874e-05 6.20158244e-06 8.96503298e-06 2.32516632e-05\n  1.11210613e-06 2.07699359e-05 1.00355559e-04 2.34511666e-04\n  1.58945704e-03 1.81522541e-04 1.96158885e-06 4.57150009e-06\n  3.24521443e-06 1.12884663e-05 8.37327461e-05 3.75008123e-04\n  9.05164779e-05 3.43701249e-05 6.27141853e-04 6.86332760e-06\n  9.29450323e-07 2.17893132e-04 1.53529290e-05 2.13491512e-04\n  4.14218848e-05 3.02345197e-05 2.65501076e-05 5.80045380e-05\n  1.40233649e-04 2.59953842e-04 2.37883123e-05 1.13902854e-06\n  2.78205334e-05 2.76170922e-05 2.47238670e-04 3.13823402e-05\n  7.30598822e-06 3.88877561e-06 9.42289444e-06 1.37904586e-04\n  1.89380342e-04 1.77044960e-04 1.48410007e-04 7.17098417e-04\n  1.58772000e-05 5.33169396e-06 4.22407102e-05 1.77481106e-05\n  5.65771961e-06 2.84052767e-05 8.31129398e-07 3.68469699e-07\n  5.04346417e-07 5.98660381e-06 8.37580160e-07 1.02045407e-04\n  2.79439118e-05 1.24850747e-04 1.84023462e-04 2.36079832e-05\n  6.47301567e-05 2.75069702e-04 4.75077759e-05 1.88719296e-05\n  5.58575084e-06 6.62346054e-07 2.94270626e-06 1.41494934e-06\n  1.10970923e-05 4.89392096e-06 1.56762042e-07 5.49019569e-06\n  3.98284101e-06 1.36403523e-05 3.08181279e-06 6.61717559e-06\n  5.72401268e-06 2.63313996e-04 2.69278644e-05 1.47796367e-04\n  1.99313901e-04 1.33720860e-05 5.08355151e-04 1.11429277e-03\n  6.93236652e-04 7.77370660e-05 5.42512105e-04 4.69845945e-05\n  1.52991284e-04 7.90421254e-05 2.05546414e-04 3.32738366e-03\n  4.63811302e-05 1.96527899e-03 7.44792706e-05 9.29388334e-05\n  7.66515077e-05 6.04281377e-05 1.15363719e-06 6.11348742e-06\n  1.98459631e-04 8.64495064e-07 3.94093193e-04 4.58976021e-04\n  1.03976581e-05 7.66750509e-06 3.56711212e-06 2.98423201e-05\n  4.25459912e-05 5.32240228e-05 5.59804266e-06 5.09161555e-06\n  4.02484432e-07 7.81681251e-07 1.06948173e-04 8.91627951e-06\n  9.07700996e-06 2.00669001e-06 1.38173584e-06 3.74828062e-07\n  2.02288291e-07 2.16631702e-06 1.92202265e-07 1.41654041e-07\n  1.09082384e-06 5.37305198e-07 4.48447082e-07 2.77453637e-07\n  2.94182627e-07 9.76371848e-06 2.55598252e-06 7.33225825e-05\n  2.91170873e-04 3.78573418e-06 1.95641660e-05 8.06710304e-05\n  4.66312969e-07 9.85689345e-04 8.03786024e-05 7.51783664e-05\n  1.78424852e-05 4.08418928e-05 3.69225154e-06 2.00946979e-05\n  2.90676326e-05 4.92036197e-05 3.82715552e-05 9.86838677e-06\n  4.92747859e-05 3.21739662e-06 3.89284833e-05 8.55132166e-05\n  6.99696648e-06 1.72922100e-05 8.01933857e-06 1.72528744e-05\n  3.44395849e-05 2.09397640e-05 8.39683980e-06 1.25584324e-04\n  2.58112705e-04 3.56726673e-07 1.51281981e-07 9.67074739e-05\n  3.40345377e-06 2.02091724e-06 2.58505988e-05 1.94899462e-06\n  1.43012667e-05 1.16891970e-04 5.35008166e-06 1.38633186e-06\n  3.00712554e-06 3.05377471e-05 2.60846748e-04 3.05595422e-05\n  6.21777377e-04 2.57200969e-04 2.91708973e-04 2.95008499e-06\n  1.59177944e-04 4.39225754e-04 2.76819969e-06 1.78525011e-06\n  1.20817151e-06 2.68096477e-03 1.64207609e-06 4.52309687e-05\n  1.87323632e-04 2.86292518e-04 2.22833132e-05 8.37728385e-06\n  2.29231222e-03 8.43088201e-05 2.53284443e-03 1.39557645e-02\n  9.71772170e-05 8.29986180e-04 1.68282695e-05 1.50987675e-04\n  8.32762635e-06 2.20613128e-05 7.79407637e-05 1.77350157e-05\n  6.01598149e-05 2.48742476e-03 1.72098044e-05 4.76037385e-05\n  1.79520110e-04 1.81021132e-05 7.42175980e-05 2.92554236e-04\n  9.17509055e-07 4.83258482e-05 1.02584265e-04 2.73878570e-04\n  7.24946076e-05 4.00731842e-05 4.85830024e-05 8.18411791e-05\n  4.91250785e-06 9.29227099e-04 3.06230533e-04 1.32321866e-05\n  8.87760194e-04 1.84146265e-05 1.96115900e-04 4.16622468e-04\n  2.08986778e-04 2.27323922e-04 1.66304142e-06 7.08933294e-05\n  3.38281691e-03 2.16722954e-03 2.52648960e-07 1.88174588e-03\n  1.01185205e-05 2.77667143e-03 5.06614510e-04 7.73696345e-04\n  2.81869288e-04 1.44683610e-04 5.71191185e-06 1.50390690e-07\n  6.33976060e-06 3.43102911e-05 8.02208111e-03 3.17127160e-05\n  7.61984993e-05 1.67895425e-02 3.89957713e-05 2.23035313e-05\n  3.67265875e-06 3.22611828e-04 8.94177065e-04 2.02201204e-06\n  2.83961763e-05 5.03512856e-04 1.82267158e-05 8.31308262e-06\n  6.61101285e-06 4.67355021e-05 3.62927420e-03 1.03019760e-03\n  5.02347830e-04 1.34924543e-04 8.13022343e-05 6.22463194e-05\n  2.55929372e-05 6.63843894e-05 2.85033067e-03 2.56390340e-05\n  6.23766985e-03 8.69870200e-06 1.82581716e-05 4.93976567e-03\n  1.50614906e-05 2.54738494e-04 2.63076799e-04 9.93223162e-04\n  6.55158190e-04 8.80664738e-04 1.43229554e-03 1.42101257e-03\n  1.85200231e-04 6.97714268e-06 5.10701429e-05 1.06867412e-06\n  2.06302218e-02 4.66152669e-05 1.32167130e-03 1.21409690e-03\n  4.50337335e-04 1.19299120e-05 7.95147032e-04 7.82047755e-06\n  3.01974924e-05 1.09823459e-05 5.08912024e-04 4.80970630e-04\n  1.48997677e-03 1.47829780e-06 4.39371724e-05 7.67702440e-05\n  1.97687317e-04 1.17719546e-03 3.32292519e-03 2.12963816e-04\n  1.02740223e-05 2.88442861e-05 1.46954417e-05 8.95229095e-05\n  8.85444251e-07 2.97654083e-07 4.27472514e-06 3.07719783e-05\n  3.83646102e-06 6.76742420e-05 3.81361810e-04 4.68132494e-04\n  2.95970563e-06 4.35143796e-04 4.10743849e-03 5.80555870e-08\n  1.68721783e-06 3.54272139e-04 2.92197044e-04 1.09322296e-04\n  2.24791584e-03 4.08249936e-04 2.35981697e-05 1.98819544e-06\n  1.53438465e-04 1.18385244e-03 1.84122741e-03 2.09200938e-04\n  4.67915146e-04 1.82168537e-06 6.69137589e-05 2.44682742e-04\n  1.55303849e-06 2.18459377e-07 2.29900470e-05 3.02416720e-05\n  2.65843883e-05 5.65506539e-08 7.36271497e-04 3.31681576e-06\n  2.43356510e-04 1.07189965e-06 2.58914218e-03 3.75690388e-06\n  1.01532614e-05 5.87176437e-05 1.01083890e-04 3.21821630e-04\n  4.19034365e-07 3.33057574e-06 1.27509441e-06 9.10745002e-05\n  1.73000135e-02 2.74249725e-03 1.43244293e-07 4.70615784e-03\n  5.16304681e-05 5.40822977e-03 1.21376172e-04 1.37223760e-04\n  6.08945338e-05 4.15740360e-04 1.84834213e-03 2.89252330e-06\n  8.11061077e-03 1.99419723e-04 4.13332564e-05 6.98154909e-04\n  1.93969682e-02 8.14256127e-05 3.64986568e-04 7.96841675e-07\n  3.56244808e-03 3.04826139e-03 5.68423979e-03 1.49688363e-04\n  7.84544754e-05 4.15758365e-07 1.35692710e-04 2.47194129e-03\n  1.06339473e-06 3.36381199e-04 2.04657990e-05 2.91961408e-03\n  2.35545426e-03 3.53888827e-05 4.62378463e-04 5.84128918e-03\n  2.00725568e-04 9.14889097e-06 5.76731145e-05 1.49362884e-03\n  4.40059694e-06 4.05724950e-06 7.31656700e-02 1.12582129e-05\n  1.52801422e-05 1.42669934e-03 1.36764080e-04 1.11735321e-03\n  6.72580791e-05 1.04693742e-03 3.14906259e-07 4.83456424e-05\n  8.70232398e-05 6.79029981e-05 3.16183595e-03 2.12409580e-03\n  7.23878713e-07 1.03803095e-03 1.15387829e-03 1.43163663e-03\n  1.25558406e-01 1.83157208e-05 3.02491426e-05 2.12946557e-04\n  6.54560514e-04 1.03072025e-05 9.47691780e-03 1.85571491e-06\n  2.22854815e-05 5.96001220e-04 1.12471753e-06 1.08179741e-03\n  1.47953460e-06 5.77171915e-04 1.48153820e-04 2.52146147e-05\n  1.62174160e-06 1.71337661e-07 2.65697774e-04 3.24378493e-06\n  1.23677892e-04 2.14539250e-05 2.96051474e-03 1.05304655e-03\n  5.52837719e-06 1.25529305e-05 1.28420448e-04 6.81834581e-06\n  1.15284583e-05 8.76759586e-04 1.16947507e-02 1.56531684e-07\n  1.89109007e-03 4.64451835e-02 2.72770907e-04 5.09046484e-04\n  3.92731657e-04 2.05786579e-04 2.69298789e-06 2.37346190e-04\n  4.14537586e-04 3.57557246e-06 2.92908899e-05 2.58841737e-05\n  4.55855225e-05 2.51993159e-04 1.67237161e-07 1.27153282e-04\n  9.58813063e-04 4.10282082e-04 8.00139023e-05 1.61289074e-03\n  2.85510533e-03 1.16051633e-04 6.30085924e-06 2.75945989e-03\n  4.68655350e-03 2.77787936e-03 6.56552089e-04 5.09704929e-04\n  1.13208635e-05 2.03645172e-07 4.52682752e-06 4.73156579e-05\n  8.80962471e-05 3.86738393e-04 1.83175486e-02 4.67828562e-04\n  1.14338245e-05 1.52213252e-05 1.26276747e-03 3.31280421e-06\n  5.19449706e-04 4.65751612e-07 2.93253129e-06 1.75705052e-03\n  5.99400513e-03 6.67717904e-05 2.58082198e-03 1.07355807e-02\n  2.14298379e-05 2.97370949e-04 4.82490839e-04 5.65240725e-06\n  7.00849399e-04 4.03788756e-04 1.94168806e-05 1.67915435e-03\n  2.02348805e-04 1.57688407e-03 6.97739210e-07 2.98554733e-05\n  1.16548908e-03 4.03904087e-05 1.65496385e-05 8.37436073e-06\n  7.17015355e-04 4.04003476e-05 9.16970675e-05 2.88644842e-05\n  3.56177014e-04 1.12371301e-04 9.23352563e-05 1.19685494e-04\n  7.94900989e-05 7.72452261e-03 2.86217019e-05 3.99824648e-06\n  1.94860005e-03 6.33972464e-04 1.34369679e-04 4.92845356e-05\n  2.71035224e-05 7.25316738e-07 2.20314570e-04 6.70591908e-05\n  1.14674267e-05 2.62334594e-03 3.13654778e-06 7.42712349e-04\n  1.36484465e-04 1.90537132e-03 7.21941115e-06 1.56383321e-03\n  1.54947033e-04 1.93791895e-03 1.16259667e-04 1.96996807e-05\n  4.76728706e-03 6.53433497e-04 1.64256571e-03 5.94181474e-05\n  6.25612302e-05 4.31586662e-03 2.42088445e-05 3.36982396e-07\n  1.85801273e-05 1.13466907e-04 3.55256838e-04 1.23734167e-02\n  3.29183321e-03 1.73497450e-04 1.43563986e-04 4.47821245e-03\n  1.32309564e-04 1.03724424e-05 3.47269161e-05 2.72573525e-04\n  3.97531607e-04 1.03368286e-04 3.07387003e-04 2.25424679e-04\n  4.48739702e-05 1.12679127e-05 3.24516659e-05 3.39818216e-05\n  3.18627135e-05 1.64335221e-03 5.76778439e-06 7.09505093e-06\n  1.04312580e-02 2.64467509e-03 2.83223955e-04 6.42025989e-05\n  6.48010682e-05 6.29120113e-05 1.95453886e-05 1.01353833e-03\n  6.93177222e-04 2.03487650e-03 8.22405200e-05 6.83725812e-05\n  3.14009783e-04 3.77951324e-07 1.28746837e-01 1.42541205e-04\n  2.27714466e-07 1.11409759e-06 5.31699698e-05 2.52346304e-04\n  9.07145950e-05 5.94351695e-06 1.44835140e-04 6.16836536e-04\n  5.14096173e-04 3.11763927e-07 1.65841746e-04 2.52286030e-04\n  1.11274915e-06 3.38506084e-06 3.40347819e-04 2.13313942e-05\n  1.09727716e-03 6.61187049e-04 1.39532145e-04 1.27724679e-05\n  9.61563841e-04 8.66707851e-05 6.44835163e-05 1.56485371e-03\n  1.90780256e-02 9.11480188e-03 3.52473231e-03 3.56918605e-07\n  1.90282535e-05 3.26302485e-04 2.82764493e-04 1.13787840e-03\n  1.85410096e-03 2.38803665e-07 3.17986473e-04 4.48740181e-03\n  3.01359387e-06 5.09109850e-05 7.48473212e-06 2.33094470e-04\n  2.97389192e-06 4.19099350e-04 9.62709449e-03 2.58501177e-06\n  4.73781910e-07 6.02077398e-06 1.13810131e-06 3.89038718e-07\n  2.03099902e-04 3.08179951e-05 6.38683414e-05 6.61459489e-06\n  4.00797580e-04 1.29717482e-05 6.85746500e-08 2.22634735e-05\n  1.95437111e-04 1.54409572e-05 1.04149658e-05 1.11315411e-03\n  8.48336713e-05 2.78561231e-04 2.07431940e-03 2.78335501e-04\n  3.91923095e-06 1.30358094e-04 1.67415910e-05 1.09847124e-05\n  1.20889363e-05 1.07830635e-03 1.70008943e-03 1.22565951e-04\n  1.26705533e-02 9.99253316e-05 8.94039331e-05 1.84681354e-04\n  1.25047553e-03 7.12193014e-06 8.79583240e-04 4.13702306e-04\n  5.39168796e-05 9.68867971e-06 5.72030526e-03 2.09657723e-04\n  6.17835962e-04 7.06255611e-04 1.64912897e-04 1.12791313e-04\n  1.33863266e-03 7.54694338e-05 7.81976560e-05 2.89864955e-04\n  5.19273635e-05 5.52342644e-05 1.66800976e-06 1.13536671e-05\n  6.61680315e-05 1.26536770e-05 3.73634648e-05 9.00285668e-04\n  6.14308694e-04 1.51655485e-03 3.77107926e-06 1.55650305e-05\n  5.35058643e-06 4.01321959e-05 3.43530161e-07 2.44429229e-05\n  5.16285727e-05 1.93517830e-03 2.02947717e-06 3.47225273e-06\n  4.10591892e-05 2.54380524e-07 2.72763227e-05 1.37610550e-05\n  1.47340606e-05 1.13215603e-04 1.72462373e-04 4.74747367e-06\n  6.79446475e-06 1.19735091e-07 1.32932528e-05 3.80273923e-05\n  1.19677788e-05 3.86783940e-05 2.92348932e-06 4.64557699e-04\n  1.48486521e-04 5.56189159e-04 6.99374068e-05 3.24296998e-05\n  9.05776142e-06 4.76820533e-05 6.60313090e-05 5.41604550e-07\n  2.30326100e-06 5.42720882e-05 1.49532832e-06 2.92716072e-06\n  3.22161563e-04 5.47524178e-06 6.85768782e-06 2.04353250e-06\n  1.18765979e-06 1.53987878e-06 1.19950942e-04 2.96407143e-05\n  1.32602325e-03 2.26359625e-04 2.08980009e-06 1.60506883e-04\n  5.67588613e-05 8.49493176e-07 4.51869710e-06 5.56861596e-05\n  2.59140484e-06 1.65575311e-05 8.14351588e-05 7.35288666e-07\n  1.13013057e-05 2.66565150e-03 1.82148833e-05 7.84839212e-04\n  4.28813619e-05 4.51437081e-04 2.10684539e-06 2.44999595e-04\n  2.10887683e-05 8.22360817e-05 1.24145256e-04 1.01523610e-05\n  4.65120029e-05 3.71207051e-07 9.47365061e-06 1.14141494e-05\n  1.41303381e-05 1.32073683e-06 1.85611323e-04 4.78283176e-03]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "###  Extract features from an arbitrary intermediate layer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_minimal = Model(input=model.input, output=model.get_layer('conv_dw_2_relu').output)\n\nconv_dw_2_relu_features = model_minimal.predict(x)\nprint('Features of conv_dw_2_relu:',conv_dw_2_relu_features.shape)",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Features of conv_dw_2_relu: (1, 56, 56, 16)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### You can extract these features and use the base network as a feature extractor for your problems. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 3: Deep Convolution Layer Visualization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib as mp\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport math",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Extract Data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Model Building"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\nx = tf.placeholder(tf.float32, [None, 784],name=\"x-in\")\ntrue_y = tf.placeholder(tf.float32, [None, 10],name=\"y-in\")\nkeep_prob = tf.placeholder(\"float\")\n\nx_image = tf.reshape(x,[-1,28,28,1])\nhidden_1 = slim.conv2d(x_image,5,[5,5])\npool_1 = slim.max_pool2d(hidden_1,[2,2])\nhidden_2 = slim.conv2d(pool_1,5,[5,5])\npool_2 = slim.max_pool2d(hidden_2,[2,2])\nhidden_3 = slim.conv2d(pool_2,20,[5,5])\nhidden_3 = slim.dropout(hidden_3,keep_prob)\nout_y = slim.fully_connected(slim.flatten(hidden_3),10,activation_fn=tf.nn.softmax)\n\ncross_entropy = -tf.reduce_sum(true_y*tf.log(out_y))\ncorrect_prediction = tf.equal(tf.argmax(out_y,1), tf.argmax(true_y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Training"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "batchSize = 50\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\nfor i in range(1001):\n    batch = mnist.train.next_batch(batchSize)\n    sess.run(train_step, feed_dict={x:batch[0],true_y:batch[1], keep_prob:0.5})\n    if i % 100 == 0 and i != 0:\n        trainAccuracy = sess.run(accuracy, feed_dict={x:batch[0],true_y:batch[1], keep_prob:1.0})\n        print(\"step %d, training accuracy %g\"%(i, trainAccuracy))",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "step 100, training accuracy 0.42\nstep 200, training accuracy 0.68\nstep 300, training accuracy 0.68\nstep 400, training accuracy 0.8\nstep 500, training accuracy 0.84\nstep 600, training accuracy 0.74\nstep 700, training accuracy 0.92\nstep 800, training accuracy 0.9\nstep 900, training accuracy 0.84\nstep 1000, training accuracy 0.84\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Testing accuracy"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "testAccuracy = sess.run(accuracy, feed_dict={x:mnist.test.images,true_y:mnist.test.labels, keep_prob:1.0})\nprint(\"test accuracy %g\"%(testAccuracy))",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": "test accuracy 0.9046\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Get activation values and plotting"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def getActivations(layer,stimuli):\n    units = sess.run(layer,feed_dict={x:np.reshape(stimuli,[1,784],order='F'),keep_prob:1.0})\n    plotNNFilter(units)\n    \ndef plotNNFilter(units):\n    filters = units.shape[3]\n    plt.figure(1, figsize=(20,20))\n    n_columns = 6\n    n_rows = math.ceil(filters / n_columns) + 1\n    for i in range(filters):\n        plt.subplot(n_rows, n_columns, i+1)\n        plt.title('Filter ' + str(i))\n        plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")",
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Input Image"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "imageToUse = mnist.test.images[0]\nplt.imshow(np.reshape(imageToUse,[28,28]), interpolation=\"nearest\", cmap=\"gray\")",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 49,
          "data": {
            "text/plain": "<matplotlib.image.AxesImage at 0x7f53555900b8>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7f53555e9208>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Activation in Layer 1"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "getActivations(hidden_1,imageToUse)",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7f53555c1f98>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Activation in Layer 2"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "getActivations(hidden_2,imageToUse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Activation in Layer 3"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "getActivations(hidden_3,imageToUse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 4: Design Choices in Convolutional Neural Networks"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Influence of convolution size"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Model with (3 x 3) Convolution"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "K.clear_session()\nstart = timeit.default_timer()   \nmodel = Sequential()\nmodel.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(16, (3, 3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\nend = timeit.default_timer()\nprint(\"Time Taken to run the model:\",end - start, \"seconds\")  ",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 24, 24, 16)        1168      \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 9216)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                294944    \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 296,522\nTrainable params: 296,522\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 183s - loss: 0.2943 - acc: 0.9122 - val_loss: 0.1137 - val_acc: 0.9666\nTime Taken to run the model: 185.2885710999999 seconds\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Model with (7 x 7) Convolution"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Write your code here \nK.clear_session()\nstart = timeit.default_timer()   \nmodel = Sequential()\nmodel.add(Conv2D(8, kernel_size=(7, 7), activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(16, (7, 7), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\nend = timeit.default_timer()\nprint(\"Time Taken to run the model:\",end - start, \"seconds\")  \n\n# Use the same model design from the above cell ",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 22, 22, 8)         400       \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 16, 16, 16)        6288      \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 4096)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                131104    \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 138,122\nTrainable params: 138,122\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 406s - loss: 0.2910 - acc: 0.9095 - val_loss: 0.1058 - val_acc: 0.9668\nTime Taken to run the model: 436.36740179999924 seconds\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Striding"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Model with (7 x 7) Convolution with 2 Steps"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "start = timeit.default_timer()   \nmodel = Sequential()\nmodel.add(Conv2D(8, kernel_size=(7, 7), strides=2, activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(16, (7, 7), strides=2, activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\nend = timeit.default_timer()\nprint(\"Time Taken to run the model:\",end - start, \"seconds\")  ",
      "execution_count": 34,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_5 (Conv2D)            (None, 11, 11, 8)         400       \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 3, 3, 16)          6288      \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 144)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 32)                4640      \n_________________________________________________________________\ndense_6 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 11,658\nTrainable params: 11,658\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 10s - loss: 0.4857 - acc: 0.8538 - val_loss: 0.2030 - val_acc: 0.9406\nTime Taken to run the model: 11.768721722642567 seconds\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "start = timeit.default_timer()   \nmodel = Sequential()\nmodel.add(Conv2D(8, kernel_size=(5, 5), strides=2, activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(16, (5, 5), strides=2, activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\nend = timeit.default_timer()\nprint(\"Time Taken to run the model:\",end - start, \"seconds\")  ",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 12, 12, 8)         208       \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 4, 4, 16)          3216      \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 256)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 32)                8224      \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 11,978\nTrainable params: 11,978\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 46s - loss: 0.4971 - acc: 0.8466 - val_loss: 0.1884 - val_acc: 0.9452\nTime Taken to run the model: 48.833171300000686 seconds\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Padding"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Model with (7 x 7) Convolution with Same Padding"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "start = timeit.default_timer()   \nmodel = Sequential()\nmodel.add(Conv2D(8, kernel_size=(7, 7), strides=1, padding='same', activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(16, (7, 7), strides=1, padding='same', activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\nend = timeit.default_timer()\nprint(\"Time Taken to run the model:\",end - start, \"seconds\")  ",
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_7 (Conv2D)            (None, 28, 28, 8)         400       \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 28, 28, 16)        6288      \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 12544)             0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 32)                401440    \n_________________________________________________________________\ndense_8 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 408,458\nTrainable params: 408,458\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 199s - loss: 0.2605 - acc: 0.9204 - val_loss: 0.0782 - val_acc: 0.9757\nTime Taken to run the model: 200.20336358687018 seconds\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "### Model with (5 x 5) Convolution with same padding",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "start = timeit.default_timer()   \nmodel = Sequential()\nmodel.add(Conv2D(8, kernel_size=(5, 5), strides=1, padding='same', activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(16, (5, 5), strides=1, padding='same', activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\nend = timeit.default_timer()\nprint(\"Time Taken to run the model:\",end - start, \"seconds\")  ",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_5 (Conv2D)            (None, 28, 28, 8)         208       \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 28, 28, 16)        3216      \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 12544)             0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 32)                401440    \n_________________________________________________________________\ndense_6 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 405,194\nTrainable params: 405,194\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 640s - loss: 0.2612 - acc: 0.9213 - val_loss: 0.0967 - val_acc: 0.9709\nTime Taken to run the model: 641.7457806000002 seconds\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Pooling"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Model with (3 x 3) Convolution with Pooling (2 x 2) "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "start = timeit.default_timer()   \nmodel = Sequential()\nmodel.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(16, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\nend = timeit.default_timer()\nprint(\"Time Taken to run the model:\",end - start, \"seconds\")  ",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_7 (Conv2D)            (None, 26, 26, 8)         80        \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 13, 13, 8)         0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 11, 11, 16)        1168      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 32)                12832     \n_________________________________________________________________\ndense_8 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 14,410\nTrainable params: 14,410\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 81s - loss: 0.4910 - acc: 0.8464 - val_loss: 0.1592 - val_acc: 0.9535\nTime Taken to run the model: 83.4051271999997 seconds\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Model with (5 x 5) Convolution with Pooling (3 x 3) "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Write your code here \nstart = timeit.default_timer()   \nmodel = Sequential()\nmodel.add(Conv2D(8, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Conv2D(16, (5, 5), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\nend = timeit.default_timer()\nprint(\"Time Taken to run the model:\",end - start, \"seconds\")  \n# Use the same model design from the above cell ",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_9 (Conv2D)            (None, 24, 24, 8)         208       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 8, 8, 8)           0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 4, 4, 16)          3216      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 1, 1, 16)          0         \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 16)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 32)                544       \n_________________________________________________________________\ndense_10 (Dense)             (None, 10)                330       \n=================================================================\nTotal params: 4,298\nTrainable params: 4,298\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 74s - loss: 0.6947 - acc: 0.7778 - val_loss: 0.1881 - val_acc: 0.9415\nTime Taken to run the model: 76.22400020000077 seconds\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### What are your findings?"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}